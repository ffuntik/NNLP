{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import wordpunct_tokenize\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from pymystem3 import Mystem\n",
    "# mystem = Mystem()\n",
    "from string import punctuation\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, word2vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.test.utils import datapath\n",
    "import tqdm\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.snowball import RussianStemmer, SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cos_sim\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"texts\"\n",
    "\n",
    "texts_all = {0: [], 1: [], 2: []}\n",
    "\n",
    "vocab = set()\n",
    "label_dict = {0: [1, 0, 0], 1: [0, 1, 0], 2: [0, 0, 1]}\n",
    "\n",
    "all_data_X = []\n",
    "all_data_Y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd42c0d434a408bb3f8b351812a95c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=68), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(directory, topdown=False):\n",
    "    for name in tqdm.tqdm_notebook(files):\n",
    "        if name.endswith(\".txt\"):            \n",
    "            file_object = open(os.path.join(root, name), encoding='utf-8')\n",
    "            read_ = file_object.read()\n",
    "            i = int(name[:4])\n",
    "            year_cat = -1\n",
    "            \n",
    "            if i < 1917:\n",
    "                year_cat = 0\n",
    "            if i < 1990 and i >= 1917:\n",
    "                year_cat = 1\n",
    "            if i < 2020 and i >= 1990:\n",
    "                year_cat = 2\n",
    "           \n",
    "            tokens = nltk.word_tokenize(read_.lower())\n",
    "            tokens = [word for word in tokens if word.isalpha()]\n",
    "            tokens = [token for token in tokens if token not in russian_stopwords and token != \" \" \\\n",
    "                          and token.strip() not in punctuation]\n",
    "            vocab.update(tokens)\n",
    "            new_file = []\n",
    "            k = 1\n",
    "            while (k*200 < len(tokens)):\n",
    "                texts_all[year_cat].append(tokens[(k-1)*200:k*200])\n",
    "                new_file.append(tokens[(k-1)*200:k*200])\n",
    "                all_data_X.append(tokens[(k-1)*200:k*200])\n",
    "                all_data_Y.append(label_dict[year_cat])\n",
    "                k += 1\n",
    "\n",
    "            fn = str(year_cat) + '.txt'    \n",
    "            with open(os.path.join(\"train_test_\", fn), 'a',  encoding='utf-8') as ff:\n",
    "                for k in range(len(new_file)):\n",
    "                    ff.write(\" \".join(new_file[k]))\n",
    "                    ff.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in all three categories is almost of the same size, so the dataset is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6597\n",
      "6664\n",
      "6289\n"
     ]
    }
   ],
   "source": [
    "print(len(texts_all[0]))\n",
    "print(len(texts_all[1]))\n",
    "print(len(texts_all[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_0 = LineSentence('train_test_/0.txt')\n",
    "sentences_1 = LineSentence('train_test_/1.txt')\n",
    "sentences_2 = LineSentence('train_test_/2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0_300_10_5 = word2vec.Word2Vec(sentences_0, size=300, window=5, workers=4, min_count=10, iter=20, sg=1)\n",
    "model_1_300_10_5 = word2vec.Word2Vec(sentences_1, size=300, window=5, workers=4, min_count=10, iter=20, sg=1)\n",
    "model_2_300_10_5 = word2vec.Word2Vec(sentences_2, size=300, window=5, workers=4, min_count=10, iter=20, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0_300_10_5.save(\"word2vec_0_300_10_5.model\")\n",
    "model_1_300_10_5.save(\"word2vec_1_300_10_5.model\")\n",
    "model_2_300_10_5.save(\"word2vec_2_300_10_5.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('прощать', 0.388449102640152),\n",
       " ('жаловать', 0.3844766616821289),\n",
       " ('заблестели', 0.3702448010444641),\n",
       " ('ненавидеть', 0.3659781217575073),\n",
       " ('деток', 0.36531126499176025),\n",
       " ('уважать', 0.3592938780784607),\n",
       " ('раскаиваться', 0.34629762172698975),\n",
       " ('горды', 0.3427136242389679),\n",
       " ('целуй', 0.3401082158088684),\n",
       " ('перестанет', 0.3394628167152405)]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_300_10_5.wv.most_similar(\"любить\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ебет', 0.47063326835632324),\n",
       " ('ненавидеть', 0.4672117233276367),\n",
       " ('жалеть', 0.4515785574913025),\n",
       " ('прощать', 0.4411221146583557),\n",
       " ('ничему', 0.42597222328186035),\n",
       " ('счастливыми', 0.42581260204315186),\n",
       " ('нищим', 0.42127496004104614),\n",
       " ('беречь', 0.4152146279811859),\n",
       " ('дурочка', 0.4144856631755829),\n",
       " ('глупее', 0.41256988048553467)]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_300_10_5.wv.most_similar(\"любить\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('смею', 0.4245392680168152),\n",
       " ('жалеть', 0.4179636836051941),\n",
       " ('желать', 0.41600197553634644),\n",
       " ('живется', 0.4051150679588318),\n",
       " ('скоты', 0.40088891983032227),\n",
       " ('упрямый', 0.39710813760757446),\n",
       " ('поверят', 0.39596107602119446),\n",
       " ('ощущать', 0.39469560980796814),\n",
       " ('пожертвовать', 0.39360636472702026),\n",
       " ('рабом', 0.3928634822368622)]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_300_10_5.wv.most_similar(\"любить\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
